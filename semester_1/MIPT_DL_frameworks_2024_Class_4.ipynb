{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\" width=\"100%\">\n",
        "    <img width=\"66%\" src=\"https://raw.githubusercontent.com/linukc/master_dlcourse/main/images/logo.png\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "QvTwr4n4Eg1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # **[MIPT DL frameworks Spring 2024](https://wiki.cogmodel.mipt.ru/s/mtai/doc/2024-nejrosetevye-frejmvorki-glubokogo-obucheniya-ZBGd69bxLd). Class 4: CUDA introduction**"
      ],
      "metadata": {
        "id": "aX4r_dZUEdsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Profiling"
      ],
      "metadata": {
        "id": "NzdR50sXM63h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "GnxR9Zk0NHlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def time_pytorch_function(func, input):\n",
        "    # CUDA IS ASYNC so can't use python time module\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(5):\n",
        "        func(input)\n",
        "\n",
        "    start.record()\n",
        "    func(input)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return start.elapsed_time(end)\n",
        "\n",
        "def square_2(a):\n",
        "    return a * a\n",
        "\n",
        "def square_3(a):\n",
        "    return a ** 2\n",
        "\n",
        "b = torch.randn(10000, 10000).cuda()\n",
        "print(time_pytorch_function(torch.square, b))\n",
        "print(time_pytorch_function(square_2, b))\n",
        "print(time_pytorch_function(square_3, b))\n",
        "\n",
        "print(\"=============\")\n",
        "print(\"Profiling torch.square\")\n",
        "print(\"=============\")\n",
        "\n",
        "# Now profile each function using pytorch profiler\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "    torch.square(b)\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
        "\n",
        "print(\"=============\")\n",
        "print(\"Profiling a * a\")\n",
        "print(\"=============\")\n",
        "\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "    square_2(b)\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
        "\n",
        "print(\"=============\")\n",
        "print(\"Profiling a ** 2\")\n",
        "print(\"=============\")\n",
        "\n",
        "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
        "    square_3(b)\n",
        "\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIQOXb6YM_RF",
        "outputId": "56f441f5-275a-4422-ea86-cb6e34927b4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.272768020629883\n",
            "3.27839994430542\n",
            "3.277600049972534\n",
            "=============\n",
            "Profiling torch.square\n",
            "=============\n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "             aten::square         0.84%      28.000us         2.89%      96.000us      96.000us      30.000us         0.89%       3.361ms       3.361ms             1  \n",
            "                aten::pow         1.38%      46.000us         1.92%      64.000us      64.000us       3.318ms        98.72%       3.331ms       3.331ms             1  \n",
            "        aten::result_type         0.03%       1.000us         0.03%       1.000us       1.000us       9.000us         0.27%       9.000us       9.000us             1  \n",
            "                 aten::to         0.00%       0.000us         0.00%       0.000us       0.000us       4.000us         0.12%       4.000us       4.000us             1  \n",
            "          cudaEventRecord         0.57%      19.000us         0.57%      19.000us       2.375us       0.000us         0.00%       0.000us       0.000us             8  \n",
            "         cudaLaunchKernel         0.30%      10.000us         0.30%      10.000us      10.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "    cudaDeviceSynchronize        96.87%       3.222ms        96.87%       3.222ms       3.222ms       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 3.326ms\n",
            "Self CUDA time total: 3.361ms\n",
            "\n",
            "=============\n",
            "Profiling a * a\n",
            "=============\n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                aten::mul         0.95%      31.000us         1.38%      45.000us      45.000us       3.310ms       100.00%       3.310ms       3.310ms             1  \n",
            "          cudaEventRecord         0.67%      22.000us         0.67%      22.000us      11.000us       0.000us         0.00%       0.000us       0.000us             2  \n",
            "         cudaLaunchKernel         0.43%      14.000us         0.43%      14.000us      14.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "    cudaDeviceSynchronize        97.95%       3.202ms        97.95%       3.202ms       3.202ms       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 3.269ms\n",
            "Self CUDA time total: 3.310ms\n",
            "\n",
            "=============\n",
            "Profiling a ** 2\n",
            "=============\n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                     Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                aten::pow         1.55%      51.000us         2.19%      72.000us      72.000us       3.325ms        99.64%       3.337ms       3.337ms             1  \n",
            "        aten::result_type         0.03%       1.000us         0.03%       1.000us       1.000us       6.000us         0.18%       6.000us       6.000us             1  \n",
            "                 aten::to         0.00%       0.000us         0.00%       0.000us       0.000us       6.000us         0.18%       6.000us       6.000us             1  \n",
            "          cudaEventRecord         0.55%      18.000us         0.55%      18.000us       3.000us       0.000us         0.00%       0.000us       0.000us             6  \n",
            "         cudaLaunchKernel         0.36%      12.000us         0.36%      12.000us      12.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "    cudaDeviceSynchronize        97.51%       3.213ms        97.51%       3.213ms       3.213ms       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 3.295ms\n",
            "Self CUDA time total: 3.337ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "# ## Default way to use profiler\n",
        "# with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
        "#     for _ in range(10):\n",
        "#         a = torch.square(torch.randn(10000, 10000).cuda())\n",
        "\n",
        "# prof.export_chrome_trace(\"trace.json\")\n",
        "\n",
        "\n",
        "## With warmup and skip\n",
        "# https://pytorch.org/docs/stable/profiler.html\n",
        "\n",
        "# Non-default profiler schedule allows user to turn profiler on and off\n",
        "# on different iterations of the training loop;\n",
        "# trace_handler is called every time a new trace becomes available\n",
        "# to visualize just open chrome://tracing and drag-and-drop the JSON file\n",
        "def trace_handler(prof):\n",
        "    print(prof.key_averages().table(\n",
        "        sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
        "    prof.export_chrome_trace(\"test_trace_\" + str(prof.step_num) + \".json\")\n",
        "\n",
        "with torch.profiler.profile(\n",
        "    activities=[\n",
        "        torch.profiler.ProfilerActivity.CPU,\n",
        "        torch.profiler.ProfilerActivity.CUDA,\n",
        "    ],\n",
        "\n",
        "    # In this example with wait=1, warmup=1, active=2, repeat=1,\n",
        "    # profiler will skip the first step/iteration,\n",
        "    # start warming up on the second, record\n",
        "    # the third and the forth iterations,\n",
        "    # after which the trace will become available\n",
        "    # and on_trace_ready (when set) is called;\n",
        "    # the cycle repeats starting with the next step\n",
        "\n",
        "    schedule=torch.profiler.schedule(\n",
        "        wait=1,\n",
        "        warmup=1,\n",
        "        active=2,\n",
        "        repeat=1),\n",
        "    on_trace_ready=trace_handler\n",
        "    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n",
        "    # used when outputting for tensorboard\n",
        "    ) as p:\n",
        "        for iter in range(10):\n",
        "            torch.square(torch.randn(10000, 10000, device=\"cuda\"))\n",
        "            # send a signal to the profiler that the next iteration has started\n",
        "            p.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzJyQB_8QvOr",
        "outputId": "e5bd3753-a4be-490d-a339-d3296830add7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::copy_         0.01%     104.000us         9.06%     172.112ms      86.056ms     171.460ms        96.34%     171.460ms      85.730ms             2  \n",
            "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     171.460ms        96.34%     171.460ms      85.730ms             2  \n",
            "                                              aten::pow         0.01%     165.000us         0.01%     244.000us     122.000us       6.515ms         3.66%       6.515ms       3.257ms             2  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       6.515ms         3.66%       6.515ms       3.257ms             2  \n",
            "                                          ProfilerStep*         2.41%      45.816ms        99.84%        1.896s     948.059ms       0.000us         0.00%     177.975ms      88.987ms             2  \n",
            "                                            aten::randn         0.00%      56.000us        88.34%        1.678s     838.878ms       0.000us         0.00%       0.000us       0.000us             2  \n",
            "                                            aten::empty         0.00%      83.000us         0.00%      83.000us      41.500us       0.000us         0.00%       0.000us       0.000us             2  \n",
            "                                          aten::normal_        88.33%        1.678s        88.33%        1.678s     838.808ms       0.000us         0.00%       0.000us       0.000us             2  \n",
            "                                               aten::to         0.00%      62.000us         9.07%     172.289ms      43.072ms       0.000us         0.00%     171.460ms      42.865ms             4  \n",
            "                                         aten::_to_copy         0.00%      57.000us         9.07%     172.227ms      86.114ms       0.000us         0.00%     171.460ms      85.730ms             2  \n",
            "                                    aten::empty_strided         0.00%      58.000us         0.00%      58.000us      29.000us       0.000us         0.00%       0.000us       0.000us             2  \n",
            "                                        cudaMemcpyAsync         9.05%     171.949ms         9.05%     171.949ms      85.975ms       0.000us         0.00%       0.000us       0.000us             2  \n",
            "                                  cudaStreamSynchronize         0.00%      59.000us         0.00%      59.000us      29.500us       0.000us         0.00%       0.000us       0.000us             2  \n",
            "                                           aten::square         0.00%      15.000us         0.01%     259.000us     129.500us       0.000us         0.00%       6.515ms       3.257ms             2  \n",
            "                                      aten::result_type         0.00%       4.000us         0.00%       4.000us       2.000us       0.000us         0.00%       0.000us       0.000us             2  \n",
            "                                       cudaLaunchKernel         0.00%      75.000us         0.00%      75.000us      37.500us       0.000us         0.00%       0.000us       0.000us             2  \n",
            "                                  cudaDeviceSynchronize         0.16%       3.080ms         0.16%       3.080ms       3.080ms       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.899s\n",
            "Self CUDA time total: 177.975ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom cpp extensions"
      ],
      "metadata": {
        "id": "omwdOfFnEpOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1' # STOP IN PLACE IF ERROR OCCUR"
      ],
      "metadata": {
        "id": "INJPbfUVWn3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wurlitzer # Capture C-level stdout/stderr pipes in Python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySmc3uRrXF0-",
        "outputId": "cc2e5d19-2927-48e3-adf3-6e91f7057dab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wurlitzer\n",
            "  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n",
            "Installing collected packages: wurlitzer\n",
            "Successfully installed wurlitzer-3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext wurlitzer"
      ],
      "metadata": {
        "id": "_86akJrUXLzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J2sy3Ca71-n",
        "outputId": "97c44915-5eff-4574-fb41-92cef7cc168a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Ninja in /usr/local/lib/python3.10/dist-packages (1.11.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/docs/stable/cpp_extension.html\n",
        "\n",
        "make sure PyTorch (>= 2.1.2) and cuda-toolkit (nvcc compiler) are installed"
      ],
      "metadata": {
        "id": "rOWV4r-VB1u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BfNTXpYD9kW",
        "outputId": "cba00bab-ad1c-4062-f340-44e5382ef6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch                            2.2.1+cu121\n",
            "torchaudio                       2.2.1+cu121\n",
            "torchdata                        0.7.1\n",
            "torchsummary                     1.5.1\n",
            "torchtext                        0.17.1\n",
            "torchvision                      0.17.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T8t-yCXEFeB",
        "outputId": "e821d3e0-e311-4670-e90d-bc2e60bf2e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternative to pybind (LSTM example - https://pytorch.org/tutorials/advanced/cpp_extension.html)"
      ],
      "metadata": {
        "id": "WOLxwXj1GmH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.cpp_extension import load_inline"
      ],
      "metadata": {
        "id": "FH0lCC4CJOD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hello world Example"
      ],
      "metadata": {
        "id": "Tjcq4n8FIRQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cpp_source = \"\"\"\n",
        "std::string hello_world() {\n",
        "  return \"Hello World!\";\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "build_directory = \"hello_world_build_dir\"\n",
        "os.makedirs(build_directory, exist_ok=True)\n",
        "\n",
        "my_module = load_inline(\n",
        "    name='my_module',\n",
        "    cpp_sources=[cpp_source],\n",
        "    functions=['hello_world'],\n",
        "    verbose=True,\n",
        "    build_directory=build_directory\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FidydbuFq3Y",
        "outputId": "36a4e198-1378-446d-dfd9-aad552039352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Emitting ninja build file hello_world_build_dir/build.ninja...\n",
            "Building extension module my_module...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading extension module my_module...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_module.hello_world())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p07fjTNrIbEZ",
        "outputId": "354884d9-3f35-4d86-9f79-39b855ca903f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sin Example"
      ],
      "metadata": {
        "id": "-BNhTdEMI5gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source = \"\"\"\n",
        "at::Tensor sin_add(at::Tensor x, at::Tensor y) {\n",
        "  return x.sin() + y.sin();\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "build_directory = \"sin_build_dir\"\n",
        "os.makedirs(build_directory, exist_ok=True)\n",
        "\n",
        "module = load_inline(name='inline_sin_extension',\n",
        "                     cpp_sources=[source],\n",
        "                     functions=['sin_add'],\n",
        "                     build_directory=build_directory)"
      ],
      "metadata": {
        "id": "6UuV08t_-Gu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import pi"
      ],
      "metadata": {
        "id": "odKeGvNPJnqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([0, 0, 0])\n",
        "y = torch.tensor([pi/2, pi, 3*pi/2])\n",
        "module.sin_add(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOsEvmu5-SLs",
        "outputId": "caa8e6b3-07df-427a-edab-b2a7bb029594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.0000e+00, -8.7423e-08, -1.0000e+00])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RGB to Grayscale Example"
      ],
      "metadata": {
        "id": "UqO4nGXy7Tmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from torchvision.io import read_image, write_png"
      ],
      "metadata": {
        "id": "01QsadJ3MLPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile grayscale_kernel.cu\n",
        "\n",
        "#include <c10/cuda/CUDAException.h>\n",
        "#include <c10/cuda/CUDAStream.h>\n",
        "\n",
        "\n",
        "__global__\n",
        "void rgb_to_grayscale_kernel(unsigned char* output, unsigned char* input, int width, int height) {\n",
        "    const int channels = 3;\n",
        "\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    if (col < width && row < height) {\n",
        "        int outputOffset = row * width + col;\n",
        "        int inputOffset = (row * width + col) * channels;\n",
        "\n",
        "        unsigned char r = input[inputOffset + 0];   // red\n",
        "        unsigned char g = input[inputOffset + 1];   // green\n",
        "        unsigned char b = input[inputOffset + 2];   // blue\n",
        "\n",
        "        output[outputOffset] = (unsigned char)(0.21f * r + 0.71f * g + 0.07f * b);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// helper function for ceiling unsigned integer division\n",
        "inline unsigned int cdiv(unsigned int a, unsigned int b) {\n",
        "  return (a + b - 1) / b;\n",
        "}\n",
        "\n",
        "\n",
        "torch::Tensor rgb_to_grayscale(torch::Tensor image) {\n",
        "    assert(image.device().type() == torch::kCUDA);\n",
        "    assert(image.dtype() == torch::kByte);\n",
        "\n",
        "    const auto height = image.size(0);\n",
        "    const auto width = image.size(1);\n",
        "\n",
        "    auto result = torch::empty({height, width, 1}, torch::TensorOptions().dtype(torch::kByte).device(image.device()));\n",
        "\n",
        "    dim3 threads_per_block(16, 16);     // using 256 threads per block\n",
        "    dim3 number_of_blocks(cdiv(width, threads_per_block.x),\n",
        "                          cdiv(height, threads_per_block.y));\n",
        "\n",
        "    rgb_to_grayscale_kernel<<<number_of_blocks, threads_per_block, 0, torch::cuda::getCurrentCUDAStream()>>>(\n",
        "        result.data_ptr<unsigned char>(),\n",
        "        image.data_ptr<unsigned char>(),\n",
        "        width,\n",
        "        height\n",
        "    );\n",
        "\n",
        "    // check CUDA error status (calls cudaGetLastError())\n",
        "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "\n",
        "    return result;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M3YB0oA7aCc",
        "outputId": "1ab4c451-ebb5-45f8-a063-89bc3e37b66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting grayscale_kernel.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_extension():\n",
        "    cuda_source = Path(\"grayscale_kernel.cu\").read_text()\n",
        "    cpp_source = \"torch::Tensor rgb_to_grayscale(torch::Tensor image);\"\n",
        "\n",
        "    build_directory = 'grayscale_build_dir'\n",
        "    os.makedirs(build_directory, exist_ok=True)\n",
        "\n",
        "    # Load the CUDA kernel as a PyTorch extension\n",
        "    rgb_to_grayscale_extension = load_inline(\n",
        "        name=\"rgb_to_grayscale_extension\",\n",
        "        cpp_sources=[cpp_source],\n",
        "        cuda_sources=[cuda_source],\n",
        "        functions=[\"rgb_to_grayscale\"],\n",
        "        with_cuda=True,\n",
        "        extra_cuda_cflags=[\"-O2\"],\n",
        "        build_directory=build_directory,\n",
        "    )\n",
        "    return rgb_to_grayscale_extension\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Use torch cpp inline extension function to compile the kernel in grayscale_kernel.cu.\n",
        "    Read input image, convert it to grayscale via custom cuda kernel and write it out as png.\n",
        "    \"\"\"\n",
        "    ext = compile_extension()\n",
        "\n",
        "    x = read_image(\"Grace_Hopper.jpg\").permute(1, 2, 0).cuda()\n",
        "    print(\"mean:\", x.float().mean())\n",
        "    print(\"Input image:\", x.shape, x.dtype)\n",
        "\n",
        "    assert x.dtype == torch.uint8\n",
        "\n",
        "    y = ext.rgb_to_grayscale(x)\n",
        "\n",
        "    print(\"Output image:\", y.shape, y.dtype)\n",
        "    print(\"mean\", y.float().mean())\n",
        "    write_png(y.permute(2, 0, 1).cpu(), \"output.png\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf9Meo6V7TCy",
        "outputId": "f9af6249-1fe2-4c07-8999-8714f732bf7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean: tensor(80.4631, device='cuda:0')\n",
            "Input image: torch.Size([606, 517, 3]) torch.uint8\n",
            "Output image: torch.Size([606, 517, 1]) torch.uint8\n",
            "mean tensor(74.3289, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean Filter Example"
      ],
      "metadata": {
        "id": "1UXGReBg60AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from torchvision.io import read_image, write_png"
      ],
      "metadata": {
        "id": "yFkDxnmjMMGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mean_filter_kernel.cu\n",
        "\n",
        "#include <c10/cuda/CUDAException.h>\n",
        "#include <c10/cuda/CUDAStream.h>\n",
        "\n",
        "\n",
        "__global__\n",
        "void mean_filter_kernel(unsigned char* output, unsigned char* input, int width, int height, int radius) {\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int channel = threadIdx.z;\n",
        "\n",
        "    int baseOffset = channel * height * width;\n",
        "    if (col < width && row < height) {\n",
        "\n",
        "        int pixVal = 0;\n",
        "        int pixels = 0;\n",
        "\n",
        "        for (int blurRow=-radius; blurRow <= radius; blurRow += 1) {\n",
        "            for (int blurCol=-radius; blurCol <= radius; blurCol += 1) {\n",
        "                int curRow = row + blurRow;\n",
        "                int curCol = col + blurCol;\n",
        "                if (curRow >= 0 && curRow < height && curCol >=0 && curCol < width) {\n",
        "                    pixVal += input[baseOffset + curRow * width + curCol];\n",
        "                    pixels += 1;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        output[baseOffset + row * width + col] = (unsigned char)(pixVal / pixels);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "// helper function for ceiling unsigned integer division\n",
        "inline unsigned int cdiv(unsigned int a, unsigned int b) {\n",
        "  return (a + b - 1) / b;\n",
        "}\n",
        "\n",
        "\n",
        "torch::Tensor mean_filter(torch::Tensor image, int radius) {\n",
        "    assert(image.device().type() == torch::kCUDA);\n",
        "    assert(image.dtype() == torch::kByte);\n",
        "    assert(radius > 0);\n",
        "\n",
        "    const auto channels = image.size(0);\n",
        "    const auto height = image.size(1);\n",
        "    const auto width = image.size(2);\n",
        "\n",
        "    auto result = torch::empty_like(image);\n",
        "\n",
        "    dim3 threads_per_block(16, 16, channels);\n",
        "    dim3 number_of_blocks(\n",
        "        cdiv(width, threads_per_block.x),\n",
        "        cdiv(height, threads_per_block.y)\n",
        "    );\n",
        "\n",
        "    mean_filter_kernel<<<number_of_blocks, threads_per_block, 0, torch::cuda::getCurrentCUDAStream()>>>(\n",
        "        result.data_ptr<unsigned char>(),\n",
        "        image.data_ptr<unsigned char>(),\n",
        "        width,\n",
        "        height,\n",
        "        radius\n",
        "    );\n",
        "\n",
        "    // check CUDA error status (calls cudaGetLastError())\n",
        "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "\n",
        "    return result;\n",
        "}"
      ],
      "metadata": {
        "id": "5hlk9VNk6-f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82f53475-4571-4541-a779-cfbe65dff29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mean_filter_kernel.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Di7o1gr8834",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "881470d5-5639-4a35-a412-fd344652b88d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input image: torch.Size([3, 606, 517]) torch.uint8\n",
            "Output image: torch.Size([3, 606, 517]) torch.uint8\n"
          ]
        }
      ],
      "source": [
        "def compile_extension():\n",
        "    cuda_source = Path(\"mean_filter_kernel.cu\").read_text()\n",
        "    cpp_source = \"torch::Tensor mean_filter(torch::Tensor image, int radius);\"\n",
        "\n",
        "    build_directory = 'mean_filter_build_dir'\n",
        "    os.makedirs(build_directory, exist_ok=True)\n",
        "\n",
        "    # Load the CUDA kernel as a PyTorch extension\n",
        "    rgb_to_grayscale_extension = load_inline(\n",
        "        name=\"mean_filter_extension\",\n",
        "        cpp_sources=cpp_source,\n",
        "        cuda_sources=cuda_source,\n",
        "        functions=[\"mean_filter\"],\n",
        "        with_cuda=True,\n",
        "        extra_cuda_cflags=[\"-O2\"],\n",
        "        build_directory=build_directory,\n",
        "    )\n",
        "    return rgb_to_grayscale_extension\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Use torch cpp inline extension function to compile the kernel in mean_filter_kernel.cu.\n",
        "    Read input image, convert apply mean filter custom cuda kernel and write result out into output.png.\n",
        "    \"\"\"\n",
        "    ext = compile_extension()\n",
        "\n",
        "    x = read_image(\"Grace_Hopper.jpg\").contiguous().cuda()\n",
        "    assert x.dtype == torch.uint8\n",
        "    print(\"Input image:\", x.shape, x.dtype)\n",
        "\n",
        "    y = ext.mean_filter(x, 8)\n",
        "\n",
        "    print(\"Output image:\", y.shape, y.dtype)\n",
        "    write_png(y.cpu(), \"output.png\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Square Matrix Example"
      ],
      "metadata": {
        "id": "_fMJmxgKLWW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CUDA kernel and C++ wrapper\n",
        "cuda_source = '''\n",
        "__global__ void square_matrix_kernel(const float* matrix, float* result, int width, int height) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < height && col < width) {\n",
        "        int idx = row * width + col;\n",
        "        result[idx] = matrix[idx] * matrix[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "torch::Tensor square_matrix(torch::Tensor matrix) {\n",
        "    const auto height = matrix.size(0);\n",
        "    const auto width = matrix.size(1);\n",
        "\n",
        "    auto result = torch::empty_like(matrix);\n",
        "\n",
        "    dim3 threads_per_block(16, 16);\n",
        "    dim3 number_of_blocks((width + threads_per_block.x - 1) / threads_per_block.x,\n",
        "                          (height + threads_per_block.y - 1) / threads_per_block.y);\n",
        "\n",
        "    square_matrix_kernel<<<number_of_blocks, threads_per_block>>>(\n",
        "        matrix.data_ptr<float>(), result.data_ptr<float>(), width, height);\n",
        "\n",
        "    return result;\n",
        "    }\n",
        "'''\n",
        "\n",
        "cpp_source = \"torch::Tensor square_matrix(torch::Tensor matrix);\"\n",
        "\n",
        "build_directory = 'square_matrix_build_dir'\n",
        "os.makedirs(build_directory, exist_ok=True)\n",
        "\n",
        "# Load the CUDA kernel as a PyTorch extension\n",
        "square_matrix_extension = load_inline(\n",
        "    name='square_matrix_extension',\n",
        "    cpp_sources=[cpp_source],\n",
        "    cuda_sources=[cuda_source],\n",
        "    functions=['square_matrix'],\n",
        "    with_cuda=True,\n",
        "    extra_cuda_cflags=[\"-O2\"],\n",
        "    build_directory=build_directory,\n",
        "    # extra_cuda_cflags=['--expt-relaxed-constexpr']\n",
        ")\n",
        "\n",
        "a = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device='cuda')\n",
        "print(square_matrix_extension.square_matrix(a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTi3Dx_yLNQh",
        "outputId": "e54f41ea-4675-4a0d-dcaf-34bc9396802b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.,  4.,  9.],\n",
            "        [16., 25., 36.]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework"
      ],
      "metadata": {
        "id": "R3bAuDPDZ2-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SparseAttention.cu - https://youtu.be/UFE6rOC4640?t=1177"
      ],
      "metadata": {
        "id": "uXrAzkXraMPz"
      }
    }
  ]
}